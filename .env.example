
# File: .env.example

# LLM Client Configuration
# Choose your LLM provider: gemini, ollama
LLM_CLIENT_TYPE=gemini

# Google OAuth Configuration
GOOGLE_CLIENT_ID=your_google_client_id_here
GOOGLE_CLIENT_SECRET=your_google_client_secret_here
JWT_SECRET=your_super_secret_jwt_key_here_make_it_long_and_random

# Gemini AI Configuration (if using LLM_CLIENT_TYPE=gemini)
GEMINI_API_KEY=your_gemini_api_key_here

# Ollama Configuration (if using LLM_CLIENT_TYPE=ollama)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama2

# GitHub Integration (when we add it)
GITHUB_CLIENT_ID=your_github_client_id
GITHUB_CLIENT_SECRET=your_github_client_secret

# Development URLs
FRONTEND_URL=http://localhost:4200
API_URL=http://localhost:3000

# Vercel Integration (when we add it)
VERCEL_TOKEN=your_vercel_token_here

# LLM Client Configuration Notes:
# 
# For Gemini (recommended for production):
# 1. Get API key from: https://makersuite.google.com/app/apikey
# 2. Set LLM_CLIENT_TYPE=gemini
# 3. Set GEMINI_API_KEY=your_actual_key
#
# For Ollama (local development, no API costs):
# 1. Install Ollama: https://ollama.ai/
# 2. Pull a model: ollama pull llama2
# 3. Set LLM_CLIENT_TYPE=ollama
# 4. Optionally set OLLAMA_BASE_URL and OLLAMA_MODEL
# 5. Make sure Ollama is running: ollama serve