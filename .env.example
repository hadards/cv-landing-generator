
# Environment Configuration
NODE_ENV=production
PORT=3000

# LLM Client Configuration
# Choose your LLM provider: gemini, ollama
LLM_CLIENT_TYPE=gemini

# Google OAuth Configuration
GOOGLE_CLIENT_ID=your_google_client_id_here
GOOGLE_CLIENT_SECRET=your_google_client_secret_here
JWT_SECRET=your_strong_jwt_secret_32_chars_min

# Gemini AI Configuration (if using LLM_CLIENT_TYPE=gemini)
GEMINI_API_KEY=gemini_api_key_here

# Ollama Configuration (if using LLM_CLIENT_TYPE=ollama)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=mistral:latest

# =======================================================
# FREE TIER OPTIMIZATIONS
# =======================================================

# Database Connection Limits (Free Tier: Supabase = 100 max connections)
DATABASE_MAX_CONNECTIONS=5
DATABASE_IDLE_TIMEOUT=30000
DATABASE_CONNECTION_TIMEOUT=3000
DATABASE_ACQUIRE_TIMEOUT=5000

# Rate Limiting (Free Tier Protection)
RATE_LIMIT_MAX_REQUESTS=20
RATE_LIMIT_WINDOW_MS=900000
GITHUB_RATE_LIMIT=20

# API Usage Limits (Gemini Free Tier Protection)
GEMINI_DAILY_LIMIT=50
GEMINI_MONTHLY_TOKEN_LIMIT=100000

# Memory Pressure Handling (Free Tier: 512MB limit)
MEMORY_PRESSURE_THRESHOLD=400

# File Cleanup (Ephemeral Storage Protection)
UPLOADS_MAX_AGE_HOURS=2
UPLOADS_MAX_FILES=50
UPLOADS_CLEANUP_INTERVAL_HOURS=1
GENERATED_MAX_AGE_HOURS=12
GENERATED_MAX_FILES=100
GENERATED_CLEANUP_INTERVAL_HOURS=2
ORPHANED_CHECK_INTERVAL_HOURS=4
ENABLE_IMMEDIATE_CLEANUP=true
POST_DOWNLOAD_CLEANUP_MINUTES=5

# Add this to your .env file
DATABASE_URL=postgresql_url_here

# GitHub Integration (when we add it)
GITHUB_CLIENT_ID=github_client_id_here
GITHUB_CLIENT_SECRET=github_client_secret_here
GITHUB_REDIRECT_URI=http://localhost:3000/api/github/callback



# CORS Configuration (keep this for backend security)
ALLOWED_ORIGINS=http://localhost:4200,http://localhost:3000

# Security Configuration
RATE_LIMIT_WINDOW_MS=900000
RATE_LIMIT_MAX_REQUESTS=100

# File Upload Configuration
MAX_FILE_SIZE=

UPLOAD_PATH=./uploads
GENERATED_PATH=./generated

# File Cleanup Configuration
UPLOADS_MAX_AGE_HOURS=24
UPLOADS_MAX_FILES=1000
UPLOADS_CLEANUP_INTERVAL_HOURS=4
GENERATED_MAX_AGE_DAYS=30
GENERATED_MAX_FILES=500
GENERATED_CLEANUP_INTERVAL_HOURS=12
ORPHANED_CHECK_INTERVAL_HOURS=24
MAX_SESSIONS_PER_USER=50

INTERNAL_PROCESSING_TOKEN=internal_processing_token_here


# LLM Client Configuration Notes:
# 
# For Gemini (recommended for production):
# 1. Get API key from: https://makersuite.google.com/app/apikey
# 2. Set LLM_CLIENT_TYPE=gemini
# 3. Set GEMINI_API_KEY=your_actual_key
#
# For Ollama (local development, no API costs):
# 1. Install Ollama: https://ollama.ai/
# 2. Pull a model: ollama pull llama2
# 3. Set LLM_CLIENT_TYPE=ollama
# 4. Optionally set OLLAMA_BASE_URL and OLLAMA_MODEL
# 5. Make sure Ollama is running: ollama serveINTERNAL_PROCESSING_TOKEN=dev_token_12345
